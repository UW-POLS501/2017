---
title: "WDI Data Analysis Example"
date: January 18, 2017
---

# Tidy and Relational Data with World Development Indicators 

This is practical example of using R to clean and tidy the World Bank [World Development Indicators](http://data.worldbank.org/data-catalog/world-development-indicators) data. This is a widely used dataset of economic, health, education, and societal indicators.

[^1]: http://data.worldbank.org/data-catalog/world-development-indicators

```{r, include=FALSE}
options(dplyr.show_progress = FALSE)
```

```{r setup,message=FALSE}
library("tidyverse")
library("stringr")
```

We first need to download the WDI data.
This could be done manually  by downloading the file and unzipping it, but these steps can also be automated with R functions.
Note that there is a [WDI](https://cran.r-project.org/package=WDI) R package, which provides a way to query the WDI database from within R, and may be better for your purposes. 
I won't use it here because dealing with CSV issues is the main purpose of this exercise.

Create a data directory if it does not already exist, and download the zip file if it does not already exists.

I'm going to define multiple variables for URL that I am downloading, and locations of files and directories where I will put stuff.
It's useful to save these as variables (and often to put them at the top of the script) for two reasons.

First, it is easier to have any "dependencies" of the script listed near the beginning - e.g. `library` statements loading packges, sourcing other R scripts with the `source` function, and at least the names of external files that you are loading. 
These are things that could or should go in the documentation or comments, but by putting them near the top and giving them understandable names, the code becomes "self-documenting". 

Second, I can resuse the variables later. I define the `dst_dir` variable and then define `dst_file` and `wdi_dir` using it rather than `dst_file <- "data/WDI_csv.zip"` and `wdi_dir <- "data/WDI"`.
That way if I want to put everything in `output`, I don't need to remember to change the other variables.
Another good practice is, if possible, to put any "constant" values near the top of a script. 
These are inputs manually entered by the user (rather than generated by the logic of the code later), and having them near the top reminds you what things you have defined.
```{r}
# URL of the zip file
wdi_url <- "http://databank.worldbank.org/data/download/WDI_csv.zip"
# location to save data
dst_dir <- "data"
# since the output directory may not exist, I should create it.
# its subdirectories may also not exist yet, so recursive = TRUE
# but it may already exists, so set showWarnings to be silent
dir.create(dst_dir, showWarnings = FALSE, recursive = TRUE)
dst_file <- file.path(dst_dir, "WDI_csv.zip")
# directory to unzip the contents of WDI_csv into
wdi_dir <- file.path(dst_dir, "WDI")
```
Now download the zip file, and unzip it into `data/WDI`.


**WARNING: this will take a few minutes since it is a large file**
But because it takes so long, only download it once.
Unzip it to `data/WDI` using the function `unzip()`.
```{r message=FALSE}
if (!file.exists(dst_file)) {
  download.file(wdi_url, dst_file)
  unzip(dst_file, exdir = wdi_dir)
}
```

## Importing and tidying data

1. What files are included in the zip file and what do they seem to correspond to.

The original page is [http://data.worldbank.org/data-catalog/world-development-indicators](here), but it doesn not include any data.

Let's see what files were included in the zip file
```{r}
dir(dst_dir)
```
The `dir` function lists the files in a directory.
Unfortunately they are all CSV files, and there is no included documentation.
However, they are clearly and consistently named. 

Since I'm lazy, I don't want to keep referring to the files as
```{r eval=FALSE}
file.path(wdi_dir, "WDI_Country.csv")
```
So I'm going to write a quick function so that I get the path the file by only giving its name.
I'm abstracting away how and where these files are stored.

```{r}
wdi <- function(name) {
  file.path(wdi_dir, str_c("WDI_", name, ".csv"))
}
```

The `file.path` function creates a path from the given components,
```{r}
file.path("/path", "to", "file")
```
It is similar, but more robust than,
```{r}
str_c("/path", "to", "file", sep = "/")
```
It is also better to `file.path` because the function name conveys the intent of the operation (you are creating a file name), in a way that `str_c` does not (you concatenating some character vectors, but it could be for anything). It also uses the correct seperator for different operatings systems.

Since there is no documentation, I open up the files and figure it out.

`WDI_Country.csv` has country-level variables. The table key is `Country Code`, and it includes names, other country codes, and various information specific to the country.
```{r message=FALSE}
wdi_country <- read_csv(wdi("Country"))
```


`WDI_Data.csv` has the bulk of the WDI data. 
The table key is the combination of a country ID (`Country Code`) and the `Indicator Code`, with the columns being years of data, and the cells being the values of those indicators.
```{r message=FALSE}
wdi_data <- read_csv(wdi("Data"))
glimpse(wdi_data)
wdi_data %>% select(1:5) %>% slice(1:2)
```
Many of these columns have "non-syntactic" names, i.e. we couldn't use them as a variable in R. 
To reference them surround them in backticks.
```{r}
select(wdi_data, `Country Code`, `Indicator Code`) %>% head()
wdi_data$`Country Code`[1:2]
```
However, if you are using `[[` (and providing a character vector of variable names) you don't need backticks,
```{r}
wdi_data[["Country Code"]][1:2]
```


`WDI_Description.csv` is unusual.
```{r message=FALSE}
read_csv(wdi("Description"))
```
It has the extension `.csv` but this looks like a text file (trust but verify file-extensions, they can lie to you); it only has one column, with a bad name.
It has a header with the date of the last data revision, and comments on dates of data revisions.

`WDI_CS_Notes.csv` seems to be `C`ountry-`S`eries data with notes for different series for each country. For example the sources used for each country.
The keys are `CountryCode` and `SeriesCode`.
```{r message=FALSE}
wdi_cs_notes <- read_csv(wdi("CS_Notes"))
wdi_cs_notes %>% select(DESCRIPTION) %>% slice(1:3) 
```

`WDI_Footnotes.csv` has notes for each *cell* in `WDI_Data.csv` (the combination of country, series, and year).
The keys are `CountryCode`, `SeriesCode`, and `Year`. 
There's a few weird things here: `Year` values are prefixed by `YR`
```{r message=FALSE}
wdi_footnotes <- read_csv(wdi("Footnotes"))
glimpse(wdi_footnotes)
wdi_footnotes$DESCRIPTION[1:3]
```

`WDI_Series.csv` has metadata on the `Series`. Notably, it has human readable descriptions of the unique series identifiers.
```{r error=TRUE,message=FALSE}
wdi_series <- read_csv(wdi("Series"))
glimpse(wdi_series)
```
Damn, that sucks. The error message `invalid multibyte string` means that there was a character in this file that was not recognized by the character encoding setting we are using.
The part `element 11` means it was character 11. 
It could also be character 10, since off the top of head I'm not sure whether this is counting from 0 or 1 (see [zero-based numbering](https://en.wikipedia.org/wiki/Zero-based_numbering)).

Stop. Right now. Watch this video.

<iframe width="560" height="315" src="https://www.youtube.com/embed/MijmeoH9LT4" frameborder="0" allowfullscreen></iframe>

Okay, now you're back and you have some idea of what a character encoding is: it is a mapping from a number to the character (glyph) to display. 
There are lots of them.
The world has more or less settled on one called [UTF-8](https://en.wikipedia.org/wiki/UTF-8), but there are still many files of different encodings floating around. 
This appears to be one of them.
By default, `read_csv` reads the data using UTF-8, and the `locale` argument controls several settings, including the encoding to use.
So, to load this data, I need to figure out two things (1) what encoding the data is in, and (2) how to use `locale()`

There is no perfect way to tell the character encoding of a file. 
When reading a file, a program can tell its *not* a certain encoding if it encounters a byte pattern that isn't used by that encoding. 
But there's no sure way to tell if it is a certain encoding.
The way this is "solved" is using an algorithm that guesses the encoding using some hueristics. 
The **readr** package includes the function `guess_encoding`:
```{r}
guess_encoding(wdi("Series"))
```
It guesses it is either [ISO-8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) (Latin1) or
[ISO-8859-2](https://en.wikipedia.org/wiki/ISO/IEC_8859-2) (Latin2)
After reading the Wikipodia pages on them, we know these are encodings that also include the accented Latin letters and other letters used in European languages that use the Latin alphabet.

See the R4DS section [11.3.2 Parsing a vector: Strings](http://r4ds.had.co.nz/data-import.html#readr-strings) for how to use locale.
I'll define a new locale using the "Latin1" encoding (equivalent to ISO-8859-1, but more descriptive),
```{r message=FALSE}
locale_latin1 <- locale(encoding = "Latin1")
wdi_series <- read_csv(wdi("Series"), locale = locale_latin1)
```
Now it works!
```{r}
glimpse(wdi_series)
```
The key for this table is `Series Code`, and it has a variety of metadata on the series. `Indicator name` is a human friendly label for the variable.


2. Load the country-level CSV file. Print out the full names of the countries. What's wrong with them Use the `readr::guess_encoding` function to find the probable encoding, adjust the `locale` to be able to load the data in the correct format.
```{r results='hide'}
wdi_country$`Long Name`
```
If you scan through the output you'll notice some names that have errors.
Here are a few country names with problems.
```{r}
wdi_country$`Long Name`[c(40, 49, 216)]
```
They should read "Republic of Côte d'Ivoire", "Curaçao", and "Democratic Republicn of São Tomé and Principé"), but instead they have symbols like `\xf4`.
A leading `\x` usually refers to a [hexadecimal number](https://en.wikipedia.org/wiki/Hexadecimal).
In this case it is the hexadecimal number of the character which the encoding can't recognize.
Often, googling for these hexadecimal numbers will let you figure out what it should be, and perhaps the encoding (you probably aren't the first person to encounter that problem).
Other ways in which encoding problems are indicated (not in R) could be with a leading `\u` for a unicode character, or the use of the symbol � or the empty box (for missing emoji). 

To figure out what the encoding is, I'll again use `guess_encoding`:
```{r}
guess_encoding(wdi("Country"))
```
This file is guessed to be [Windows-1252](https://en.wikipedia.org/wiki/Windows-1252) (sometimes called CP-1252 where CP stands for "code point").
But that is different than the previous file, which was encoded as ISO-1859-1.

WTF? Are the files encoded differently? 
Now I'm paranoid. I should check all the CSV files to see their encodings.
```{r}
for (filename in dir(wdi_dir, full.names = TRUE)) {
  cat(filename, "\n")
  print(guess_encoding(filename))
}
```
Bug after reading through the Wikpedia page and some additional Googling, I learn that Windows-1252 is a *superset* of ISO-1859-1.
This means that we lose nothing by specifying Windows-1252, and a few files must use one of the characters that appears in Windows-1252 and not ISO-1859-1.
So I can set the encoding to `Windows-1252` for all the CSV files.

This may seem terribly complicated, and I don't envy the engineers who have had to figure out and maintain all this.
But, what you just learned will cover 99% of all your encoding issues as long as you are working with English or Western European language texts.

Try UTF-8. If there are problems with loading it using the default UTF-8, it was probably created by some Windows program that used Windows-1252.
Try that. 
If Windows-1252 doesn't work, then you have a problem.
However, you are working with Chinese, Japanese, Arabic, Cyrillic, ... that's another story).

As before, I'll create a new `locale` object with Windows-1252 encoding.
```{r}
locale_windows1252 <- locale(encoding = "Windows-1252")
```
I'll need to reload all the files using that new encoding.
I could load each file separately,
```{r message=FALSE}
wdi_country <- read_csv(wdi("Country"), locale = locale_windows1252)
wdi_data <- read_csv(wdi("CS_Notes"), locale = locale_windows1252)
# ... and so on
```
But this is seeming redundant, I am continually calling `read_csv(wdi(name), locale = locale_windows1252)`.
As R4DS says, if you copy something three or more times, you should write a function.
So here it is,
```{r}
read_wdi_csv <- function(file, ...) {
  read_csv(file, ..., locale = locale(encoding = "Windows-1252"))
}
```
The `...` passes arguments to `read_csv`, which is useful if it turns out that a specific file needs to be treated differently.
Now I can do this:
```{r message=FALSE}
wdi_country <- read_wdi_csv(wdi("Country"))
wdi_cs_notes <- read_wdi_csv(wdi("CS_Notes"))
wdi_data <- read_wdi_csv(wdi("Data"), progress = FALSE)
wdi_description <- read_wdi_csv(wdi("Description"))
wdi_footnotes <- read_wdi_csv(wdi("Footnotes"))
wdi_series <- read_wdi_csv(wdi("Series"))
wdi_st_notes <- read_wdi_csv(wdi("ST_Notes"))
```

But even that seems annoying and redundant. 
I already have a list of filenames, and I want to name them similarly to their filenames. 
I will load them using the same function, so I really should be using a loop, which would look something like this.
```
for (filename in filelist) {
   nameofvar <- read_csv(filename, locale = locale_windows1252
}
```
I can get the names of the files using `dir`.
But how do I programmatically generate the variable names. 
The answer is to put them in a list, and then refer to the list.
```{r message=FALSE}
# create
filenames <- dir(wdi_dir, full.names = TRUE)
WDI <- vector("list", length(filenames))
wdi_names <- vector("character", length(filenames))
# for filename in filenames is too confusing
for (i in seq_along(filenames)) {
  fn <- filenames[i]
  # get only the file-part of the file
  name <- basename(fn)
  # extract the name part
  name <- str_match(name, "WDI_(.*)\\.csv")[1, 2]
  # I want the names to be lower-cased
  name <- str_to_lower(name)
  WDI[[i]] <- read_wdi_csv(fn, progress = FALSE)
  wdi_names[[i]] <- name
}
names(WDI) <- wdi_names
```
*Note* I had to adjust this code several times to get the looping and naming correct. 

To see what was going on inside the loop to generate the names of the lis elements:
```{r}
fn <- "data/WDI/WDI_Country.csv"
name <- basename(fn)
print(name)
str_match(name, "WDI_(.*)\\.csv")
name <- str_match(name, "WDI_(.*)\\.csv")[1, 2]
name
str_to_lower(name)
```

Now I have a list of tibbles, with names based off their CSV filenames.
```{r}
names(WDI)
```
```{r}
glimpse(WDI[["country"]])
```

This may have been overkill for this particular case.
It is also the result of me cleaning data for many years so I've developed certain generalizable patterns.
But where you can, start to use parts of the pipeline above, such as writing functions for reusable parts of the data-processing pipeline, Loading files in a loop or iteratively rather than copying and pasting.


4. Is this tidy data? Why or why not? Why do you think they organized the data this way?

5. Create a tidy dataset in which the rows correspond to (country, year) tuples, and `country`, `year`, and all the indicators are columns.

It's not tidy. If the rows should be (country-year) observations, and the columns the series. 

This creates a data-frame with four columns: country, series, year, and the value.
```{r}
wdi_data_cst <-
  WDI$data %>%
  select(-`Indicator Name`, `Country Name`) %>%
  gather(year, value, matches("^[0-9][0-9][0-9][0-9]$"),
         na.rm = TRUE, convert = TRUE)

```
We'll see later that this format can be useful for (1) simultaneously summarizing many varaibles, or (2) plotting multiple variables in ggplot.

When selecting the columns to gather I used the `matches` function which selects columns by a regular expression pattern (See the R4DS chapter [Strings](http://r4ds.had.co.nz/strings.html)).
In this case, I wanted to select columns which had names consisting of four-numbers.

There are a few other options I could have used for the `gather` expression. 
This selects everything but the country and indicator variables.
```{r eval=FALSE}
gather(year, value, -`Country Name`, -`Indicator Code`)
```
But what if I didn't drop `Indicator Name`, and `Country Name`? The code would break.

You may be wondering why I dropped `Indicator Name` and `Country Name`.
I dropped `Indicator Name` because it would not work when I would need to spread the data to make the series into separate columns.
I dropped `Country Name` since later, I will merge the country data from `WDI_Country.csv`, and that has multiple country name variables.

So I could have manually entered the years, and done so using a range:
```{r eval=FALSE}
gather(year, value, `1960`:`2016`) 
```
However, this requires that I know the range of years, and it could break if I ran the code again next year, when there will be a column for 2017.

But why did the World Bank set up the data like this? 
Recall the other files that are provided.
Some of them have metadata on series, country-series, and countries.
With the data in the original format we can merge series metadata
from `WDI_Series` with `WDI_Data`:
```{r results='hide'}
left_join(WDI$data, WDI$series,
          by = c("Indicator Code" = "Series Code"))
```
We couldn't do that if the years were in rows, and the series were seperate columns. 
So for some manipulations, the original format is better, and for others the tidy format will be better.
What is not cool is naming the column "Indicator Code" in one data set and "Series Code" in the other.

I specified the option `na.rm = TRUE` to `gather()` in order to drop missing values since this is a larger dataset.

To make the tidy dataset, call `spread` in order to make a column for each value in `Indicator Code`, filled in with the numbers in the `value` column.
```{r}
wdi_data_tidy <- wdi_data_cst %>%
  spread(`Indicator Code`, value)
dim(wdi_data_tidy)
```


5. Merge the tidy data with the country-level data. 

```{r}
wdi_data_tidy <- 
  left_join(wdi_data_tidy, WDI$country, by = "Country Code")
```
Sometimes you have to worry about whether the indicators are the same or if everything

```{r}
wdi_data_tidy %>%
  count(`Country Code`) %>%
  anti_join(WDI$country, by = "Country Code")
```

By the way, although `left_join` will merge two data frames using any variables that appear in both, you should *always* explicitly specify an argument in `by`. 
There are at least two reasons to do this: (1) it makes the code self-documenting.
Which gives you a better idea of what the code is doing?
```{r eval=FALSE}
left_join(wdi_data_tidy, WDI$country)
```
or
```{r eval=FALSE}
left_join(wdi_data_tidy, WDI$country, by = "Country Code")
```
In the former, I'd probably include a comment as to what I was merging on:
```{r eval=FALSE}
# merge the tidy data to country level data by country codes
left_join(wdi_data_tidy, WDI$country)
```
But that is not a good comment. 
If code is clearly written, it should convey *what* it is doing.
The example with `by = "Country Code"` accomplishes that.
And even unclear code, if you spend enough time working through it, you should be able to find out *what* it does.
Comments should primarily be used for explainging *why* you are doing it.
```{r eval=FALSE}
# get descriptive country names for labelling plots and the income categories
left_join(wdi_data_tidy, WDI$country, by = "Country Code")
```
I may be able to infer why I merged country data from what I do with it later, but this is harder to understand.

   
6. The WDI data include "country" observations that are actually aggregations of countries, for example, "OECD members", "High income", "Sub-Saharan Africa (all income levels)". Filter out those observations. Hint: there is a variable that is missing for non-countries, so you shouldn't need to enumerate all of the non-country values.

Once I realized that the WDI data included non-countries, I guessed there must be an indicator in the country data which I could use to filter out non-countries.
Unforuntately, there wasn't a variable that directly measured this.
So then I started looking for variables that only make sense for countries and would be missing for the aggregates.
I found that `Income Group` was missing for all the aggregates, and non missing for countries.
```{r}
filter(WDI$country, is.na(`Income Group`))[["Short Name"]]
```

Now that I've found a variable that identifies the non-countries, I can filter them out.
This is a case where I'd want to make a comment in a script as to why I was doing this, because the thing I want to accomplish is not at all obvious from what I'm doing in the code.
In a few months (or hours) I'd look back and wonder, why was I dropping observations with a missing income group, especially if the analysis never ends up using that variable.
```{r}
wdi_data_tidy <- 
  # remove aggregate rows (e.g. Euro Area, World, etc) and
  # keep only countries. Only countries have non-missing `Income Group`
  filter(wdi_data_tidy, !is.na(`Income Group`))
```


7. Choose a few variables you are interested. 

    - Summarize the distributions of these variables by year and income group: mean, median, min, max, standard deviation, 25th percentile, 75th percentile. Accomplish by first using `gather` to create a data frame with columns: `country`, `year`, `indicator`, and `value`. Then you can use `group_by()` and `summarize()` to summarize the data.
    
    - Accomplish the same task using `mutate_at` without using `gather`
    
    - Create a line plot in which the x-axis is years, the y-axis is the mean of the indicator, each income group has its own line and a different color, and faceted by indicator. Merge the indicator level dataset in order to give the indicators meaningful labels in the plot.
      



## Gender Inequality

The UN has its own measures of Gender Inequality.
Some other simple measures that often appear may be the difference in % of males and females in the labor force, or % difference in educational attainment.

What I'm going to do is find all variables in the WDI that are disaggregated into male and female, and then calculate the difference in those measures.
But there are
```{r}
nrow(WDI$series)
```
indicators in the WDI.
I could search for male and female [online](http://databank.worldbank.org/data/reports.aspx?source=world-development-indicators), then manually put together a list, probably in a CSV file with the indicator code for the male and female variables.
Again, that's a lot of work, and I'm not being paid by the hour.

The purpose of this exercise is not to produce good measures of gender inequality, but to compute on data about the series themselves to find and match male and female variables to create the sort of dataset that could be used to construct such an index.

The plan is to see whether male and female variables have regular codes or names.
If there is some pattern, I can select the matching male and female variables.
Let's look at variables with "female" in the description. 
I use the regexp pattern `"\\bfemale\\b"`, where `\b` means the boundary of a word (space, punctuation, etc). If I didn't do that it could pick up words with female as part of the name; I can't think of any, but I'm paranoid.
```{r}
WDI$series %>%
  filter(str_detect(`Indicator Name`, "\\bfemale\\b")) %>% select(`Indicator Name`, `Series Code`)
```
It seems that the series codes of female variables generally have an `FE` in them, e.g. `"SP.DYN.LE00.FE.IN"` or `"SL.TLF.PART.FE.ZS"` or `"IC.REG.PROC.FE"`.

What about male variables? Now including the word boundary indicators is very important, or searching for names with the string "male" would also find all names with the string "female".
```{r}
str_detect("Firms with female top manager (% of firms)", "male")
```
```{r}
str_detect("Firms with female top manager (% of firms)", "\\bmale\\b")
```

So let's search for the male variables,
```{r}
WDI$series %>%
  filter(str_detect(`Indicator Name`, "\\bmale\\b")) %>%
  select(`Series Code`, `Indicator Name`)
```
It seems that the male variables, have names that include an `MA` in them.

So one approach is the find all the female variables by finding variables with `FE` in their code, replace that with `MA` and match it to the male variable names.

I will use the pattern `"\\bFE\\b"` since the `FE` appears either between periods, `.FE.` or at the end of the code `.FE`. 
If there are any other codes that include the substring `FE` I don't want to detect them.
```{r}
gender_variables <-
  WDI$series %>%
  filter(str_detect(`Series Code`, "\\bFE\\b")) %>%
  select(female_series = `Series Code`,
         female_name = `Indicator Name`)
```
One way to check that this worked is to look for any variables that don't have the word "female" in their name.
```{r}
filter(gender_variables, !str_detect(female_name, "\\bfemale\\b"))
```
The four variables don't have female in their name because they use the word "women" instead, so that's okay.
I should also manually scan through the names of the variables, because there could always be something about the data that I didn't consider. 
Likewise, for the variables that aren't captured.
But I'm going to move on knowing that this may not be perfect, but it's a pretty good first cut.
It finds
```{r}
nrow(gender_variables)
```
female variables.

Now let's find the corresponding male variables.
```{r}
gender_variables <-
  gender_variables %>%
  mutate(male_series = str_replace(female_series, "\\bFE\\b", "MA"))
```
Now merge with the original dataset to ensure that those variables actually exist, because we should assume nothing (or assume that the data wants to make your life miserable),
```{r}
gender_variables <-
  gender_variables %>%
  left_join(select(WDI$series, 
                   male_series = `Series Code`, 
                   male_name = `Indicator Name`),
                   by = "male_series")
```
Female series which didn't have a male series will have a missing value for `male_name`,
```{r}
filter(gender_variables, is.na(male_name))
```
It looks like these variables mostly shares, so the male value is implicit (100 - female value).
I'll drop and ignore them for now, but there are not many of these, and I could handle them separately as special cases.
```{r}
gender_variables <-
  filter(gender_variables, !is.na(male_name))
```

What I want to end up with is a dataset with columns:

- country
- year
- indicator code (I'll need to define my own)
- male value
- female value
- difference (female - male)

Add an overall series name that replaces `FE` and `MA` with a double-underscore (`__`).
I know `__` doesn't appear anywhere in series names, so it's easy to replace that pattern to get back ther original codes.
```{r}
gender_variables <-
  mutate(gender_variables, series = str_replace(female_series, "\\bFE\\b", "__"))
```
Reshape it to be long
```{r}
gender_series_codes <-
  gender_variables %>%
  select(series, male_series, female_series) %>%
  gather(variable, `Series Code`, -series) %>%
  separate(variable, c("gender", "code")) %>%
  select(-code)
```

Now, we can take the long WDI with columns country, year, series, value, and extract only those series related to gender.
Then create separate columns for male and female values.
And calculate the difference in values between female and male indicators.
```{r}
left_join(gender_series_codes, wdi_data_cst,
          by = c("Series Code" = "Indicator Code")) %>%
  select(-`Series Code`) %>%
  spread(gender, value) %>%
  mutate(gender_diff = female - male) %>%
  filter(gender_diff != 0)
```

This is not a great methodology. 
Calculating a difference is likely inapproriate for many variables. 
We would want to rescale the differences to the spread of the distribution of these values because currently there is no sense of what is a big difference.
But everything from here on out requires more domain expertise.
The point of this exercise was to give an example of computing on variable metadata to select and match variables to each other.


## Transparency and Missing Values

Hollyer et al. (2011) calculates a measure of governmental transparency using the number of missing values in the WDI.[^hollyer]
We'll calculate something similar.

For each country-year calculate a similar figure. Calculate a similar index of missingness. Consider the following: would you use all variables? If not, how would you select those variables? Would you calculate missing values in all variables and all time periods equally?

The WDI groups variables into topics.
I want to calculate the missingness for each country within a different topic, but those provided in `Topics` are more specific than I'd like. 
I want to extract the top-level topic and use that: e.g. "Economic Policy & Debt" rather than "Economic Policy & Debt: Balance of payments: Capital & financial account".

I'll create a dataset of unique topics
```{r}
topics <- WDI$series %>% select(Topic) %>% distinct()
```
I'll split the variable Topic by `:`:
```{r}
head(str_split(topics$Topic, ":"))
```
The problem is that there is an extra space after the `:`, so the other topics.
Since `str_split` returns a list, I'll use `map` to apply it to each element in the list.
```{r}
map(str_split(topics$Topic, ":"), str_trim) %>% head()
```
Looks good. The extra spaces are gone. Now that it is working, 
I can add this as a list column to the `topics` dataset.
```{r}
topics <-
  mutate(topics, sub_topics = map(str_split(Topic, ":"), str_trim))
```
I would also like the main (first) topic as a character vector.
I can extract that using a map function.
I use `map_chr` since I want it to return a character vector rather than a list.
```{r}
map_chr(topics$sub_topics, function(x) x[[1]]) %>%
  head()
```
Since extracting an element is such a common case, `map` has a shortcut.
If you use a number or a character vector, it will extract that element.
This will do the same thing as above:
```{r}
map_chr(topics$sub_topics, 1) %>% head()
```
Now that that's working, let's add it as a 
```{r}
topics <-
  mutate(topics, main_topic = map_chr(sub_topics, 1))
```
Now the `topics` are organized by their main topic, and we also have a variable to access the full hierarchy if need be.
```{r}
glimpse(topics)
```
```{r}
count(topics, main_topic)
```

Let's merge the topic data back to the series level data.
I will create a new dataset so that the data in `WDI` always remains the original data.
```{r}
series <- left_join(WDI$series, topics, by = "Topic")
```
Let's count the number of series by main topic,
```{r}
count(series, main_topic, sort = TRUE)
```

Now take the country-year-series dataset, `wdi_data_cst`, merge it with the series data, and for each country year, aggregate the proportion missing within each category.

```{r}
missing <- 
  left_join(wdi_data_cst, 
          # keep only the variables I'm going to use to save space:
          select(series, `Series Code`, Topic, main_topic),
          by = c("Indicator Code" = "Series Code")) %>%
  group_by(`Country Code`, main_topic, year) %>%
  mutate(missing = is.na(value)) %>%
  summarise(prop_missing = mean(missing))
summary(missing$prop_missing) 
```
Something's wrong. Oh yeah, I deleted all the missing rows when a used `gather` to create `wdi_data_cst` because it seemed like a good idea at the time.
Now I can go back, and adjust my previous code, but that could break previous things. 
Instead, I'll start from `WDI$data` and 

```{r}
missingness <-
  WDI$data %>%
  select(-`Indicator Name`, -`Country Name`) %>%
  gather(year, value, matches("^\\d+$")) %>%
  left_join(select(series, `Series Code`, Topic, main_topic),
            by = c("Indicator Code" = "Series Code")) %>%
  group_by(`Country Code`, main_topic, year) %>%
  mutate(missing = is.na(value)) %>%
  summarise(prop_missing = mean(missing))
summary(missingness$prop_missing)  
```
That looks better. Now there are missing values for each country.

However, WDI indicators have varying coverage over time, both due to the capabilities and decisions of the coverage and also structural missingnness (the UN was not collecting that indicator at that time). We should drop any indicator that is missing for *all* countries in a year. If the WDI isn't tracking it that year, we shouldn't hold it against the country.

We can edit the previous code by adding a grouped filer to remove
any indicator years where there are no non-missing values.
```{r}
missingness <-
  WDI$data %>%
  select(-`Indicator Name`, -`Country Name`) %>%
  gather(year, value, matches("^\\d+$")) %>%
  left_join(select(series, `Series Code`, Topic, main_topic),
            by = c("Indicator Code" = "Series Code")) %>%
  ### New code
  ### remove indicators which are missing for all countries in a year
  group_by(`Indicator Code`, year) %>%
  filter(!all(is.na(value))) %>%
  ###
  group_by(`Country Code`, main_topic, year) %>%
  mutate(missing = is.na(value)) %>%
  summarise(prop_missing = mean(missing),
            n_indicators = n())
summary(missingness$prop_missing)  
```


```{r}
missingness %>%
  group_by(main_topic, year) %>%
  summarise(prop_missing = mean(prop_missing)) %>%
  ggplot(aes(x = as.integer(year), y = prop_missing, colour = main_topic)) +
    geom_line()

```

The top 10 countries with most missing economic values:
```{r}
missingness %>%
  filter(main_topic == "Economic Policy & Debt",
         year == 2012) %>%
  top_n(10, desc(prop_missing)) %>%
  arrange(prop_missing)
```
and least missing
```{r}
missingness %>%
  filter(main_topic == "Economic Policy & Debt",
         year == 2012) %>%
  top_n(10, desc(prop_missing)) %>%
  arrange(prop_missing)
```



[^hollyer]: See the [HRV Transparency Project](http://0001c70.wcomhost.com/wp2/papers/) for more recent  information on it; a 2014 paper of theirs in *Political Analysis* provides a more rigorous measure. 


## Joining WDI to VDEM data

The Varieties of Democracy project is a recent project to develop indicators of democracy.  We would like to merge it with the WDI indicators. Download [Version 6.2](https://www.v-dem.net/en/data/data-version-6-2/) County-Year dataset.

```{r}



```

If this were a script, I would have put these constants at the top of the file, but in this document, they fit here better.
```{r}
vdem_url <- "http://v-dem.pol.gu.se/v6.2/Country_Year_V-Dem_CSV_v6.2.zip" 
# basename gets only the filename part
vdem_zip <- file.path(dst_dir, basename(vdem_url))
```

```{r message=FALSE,results='hide'}
if (!file.exists(vdem_zip)) {
  download.file(vdem_url, vdem_zip)
  vdem_files <- unzip(vdem_zip, exdir = dst_dir)
}
vdem_dir <- file.path(dst_dir, "Country_Year_V-Dem_CSV_v6.2")
vdem_file <- file.path(vdem_dir, "V-Dem-DS-CY-v6.2.csv")
```

What's in the zip file?
```{r}
dir(vdem_dir)
```
The file names are all self-explanatory so there's not much else to write here.
The only CSV file is the one with the data:
```{r results='hide'}
vdem <- read_csv(vdem_file)
```
with `read_csv` there are a lot of parsing errors.
The trailing characters is likely because the 
```{r}
problems(vdem)
```

What are the primary VDEM indices? Subset the data to include only those indices and country and year identifiers.

Merge the VDEM and WDI data. What does VDEM use as its country-level indicator? What does the WDI use? Note: The **countrycode** package may be useful here.



