---
title: "WDI Data Analysis Example"
date: January 18, 2017
---

# Tidy and Relational Data with World Development Indicators 

This is practical example of using R to clean and tidy the World Bank [World Development Indicators](http://data.worldbank.org/data-catalog/world-development-indicators) data. This is a widely used dataset of economic, health, education, and societal indicators.

[^1]: http://data.worldbank.org/data-catalog/world-development-indicators


```{r message=FALSE}
library("tidyverse")
library("stringr")
```

We first need to download the WDI data.
This could be done manually  by downloading the file and unzipping it, but these steps can also be automated with R functions.
Note that there is a [WDI](https://cran.r-project.org/package=WDI) R package, which provides a way to query the WDI database from within R, and may be better for your purposes. 
I won't use it here because dealing with CSV issues is the main purpose of this exercise.

Create a data directory if it does not already exist, and download the zip file if it does not already exists.

I'm going to define multiple variables for URL that I am downloading, and locations of files and directories where I will put stuff.
It's useful to save these as variables (and often to put them at the top of the script) for two reasons.

First, it is easier to have any "dependencies" of the script listed near the beginning - e.g. `library` statements loading packges, sourcing other R scripts with the `source` function, and at least the names of external files that you are loading. 
These are things that could or should go in the documentation or comments, but by putting them near the top and giving them understandable names, the code becomes "self-documenting". 

Second, I can resuse the variables later. I define the `dst_dir` variable and then define `dst_file` and `wdi_dir` using it rather than `dst_file <- "data/WDI_csv.zip"` and `wdi_dir <- "data/WDI"`.
That way if I want to put everything in `output`, I don't need to remember to change the other variables.
Another good practice is, if possible, to put any "constant" values near the top of a script. 
These are inputs manually entered by the user (rather than generated by the logic of the code later), and having them near the top reminds you what things you have defined.
```{r}
# URL of the zip file
wdi_url <- "http://databank.worldbank.org/data/download/WDI_csv.zip"
# location to save data
dst_dir <- "data"
# since the output directory may not exist, I should create it.
# its subdirectories may also not exist yet, so recursive = TRUE
# but it may already exists, so set showWarnings to be silent
dir.create(dst_dir, showWarnings = FALSE, recursive = TRUE)
dst_file <- file.path(dst_dir, "WDI_csv.zip")
# directory to unzip the contents of WDI_csv into
wdi_dir <- file.path(dst_dir, "WDI")
```
Now download the zip file, and unzip it into `data/WDI`.

**WARNING: this will take a few minutes since it is a large file**
But because it takes so long, only download it once.
Unzip it to `data/WDI` using the function `unzip()`.
```{r}
if (!file.exists(dst_file)) {
  download.file(wdi_url, dst_file)
  unzip(dst_file, exdir = wdi_dir)
}
```

## Importing and tidying data

1. What files are included in the zip file and what do they seem to correspond to.

The original page is [http://data.worldbank.org/data-catalog/world-development-indicators](here), but it doesn not include any data.

Let's see what files were included in the zip file
```{r}
dir(dst_dir)
```
The `dir` function lists the files in a directory.
Unfortunately they are all CSV files, and there is no included documentation.
However, they are clearly and consistently named. 

Since I'm lazy, I don't want to keep referring to the files as
```{r eval=FALSE}
file.path(wdi_dir, "WDI_Country.csv"
```
So I'm going to write a quick function so that I get the path the file by only giving its name.
I'm abstracting away how and where these files are stored.
```{r}
wdi <- function(name) {
  file.path(wdi_dir, str_c("WDI_", name, ".csv"))
}
```

The `file.path` function creates a path from the given components,
```{r}
file.path("/path", "to", "file")
```
It is similar, but more robust than,
```{r}
str_c("/path", "to", "file", sep = "/")
```
It is also better to `file.path` because the function name conveys the intent of the operation (you are creating a file name), in a way that `str_c` does not (you concatenating some character vectors, but it could be for anything). It also uses the correct seperator for different operatings systems.

Since there is no documentation, I open up the files and figure it out.

`WDI_Country.csv` has country-level variables. The table key is `Country Code`, and it includes names, other country codes, and various information specific to the country.
```{r}
wdi_country <- read_csv(wdi("Country"))
glimpse(wdi_country)
head(wdi_country)
```


`WDI_Data.csv` has the bulk of the WDI data. 
The table key is the combination of a country ID (`Country Code`) and the `Indicator Code`, with the columns being years of data, and the cells being the values of those indicators.
```{r}
wdi_data <- read_csv(wdi("Data"))
glimpse(wdi_data)
wdi_data %>% select(1:5) %>% slice(1:2)
```
Many of these columns have "non-syntactic" names, i.e. we couldn't use them as a variable in R. 
To reference them surround them in backticks.
```{r}
select(wdi_data, `Country Code`, `Indicator Code`) %>% head()
wdi_data$`Country Code`[1:2]
```
However, if you are using `[[` (and providing a character vector of variable names) you don't need backticks,
```{r}
wdi_data[["Country Code"]][1:2]
```


`WDI_Description.csv` is unusual.
```{r}
read_csv(wdi("Description"))
```
It has the extension `.csv` but this looks like a text file (trust but verify file-extensions, they can lie to you); it only has one column, with a bad name.
It has a header with the date of the last data revision, and comments on dates of data revisions.

`WDI_CS_Notes.csv` seems to be `C`ountry-`S`eries data with notes for different series for each country. For example the sources used for each country.
The keys are `CountryCode` and `SeriesCode`.
```{r}
wdi_cs_notes <- read_csv(wdi("CS_Notes"))
wdi_cs_notes %>% select(DESCRIPTION) %>% slice(1:3) 
```

`WDI_Footnotes.csv` has notes for each *cell* in `WDI_Data.csv` (the combination of country, series, and year).
The keys are `CountryCode`, `SeriesCode`, and `Year`. 
There's a few weird things here: `Year` values are prefixed by `YR`
```{r}
wdi_footnotes <- read_csv(wdi("Footnotes"))
glimpse(wdi_footnotes)
wdi_footnotes$DESCRIPTION[1:3]
```

`WDI_Series.csv` has metadata on the `Series`. Notably, it has human readable descriptions of the unique series identifiers.
```{r error=TRUE}
wdi_series <- read_csv(wdi("Series"))
glimpse(wdi_series)
```
Damn, that sucks. The error message `invalid multibyte string` effectively means there was a character in this file that was not recognized by the character encoding setting we are using.
The part `element 11` means it was character 11. 
It could also be character 10, since off the top of head I'm not sure whether this is counting from 0 or 1 (see [zero-based numbering](https://en.wikipedia.org/wiki/Zero-based_numbering)).

Stop. Right now. Watch this video.

![](https://www.youtube.com/watch?v=MijmeoH9LT4)

Okay, now you're back and you have some idea of what a character encoding is: it is a mapping from a number to the character (glyph) to display. 
There are lots of them.
The world has more or less settled on one called [UTF-8](https://en.wikipedia.org/wiki/UTF-8), but there are still many files of different encodings floating around. 
This appears to be one of them.
By default, `read_csv` reads the data using UTF-8, and the `locale` argument controls several settings, including the encoding to use.
So, to load this data, I need to figure out two things (1) what encoding the data is in, and (2) how to use `locale()`

There is no perfect way to tell the character encoding of a file. 
When reading a file, a program can tell its *not* a certain encoding if it encounters a byte pattern that isn't used by that encoding. 
But there's no sure way to tell if it is a certain encoding.
The way this is "solved" is using an algorithm that guesses the encoding using some hueristics. 
The **readr** package includes the function `guess_encoding`:
```{r}
guess_encoding(wdi("Series"))
```
It guesses it is either [ISO-8859-1](https://en.wikipedia.org/wiki/ISO/IEC_8859-1) (Latin1) or
[ISO-8859-2](https://en.wikipedia.org/wiki/ISO/IEC_8859-2) (Latin2)
After reading the Wikipodia pages on them, we know these are encodings that also include the accented Latin letters and other letters used in European languages that use the Latin alphabet.

See the R4DS section [11.3.2 Parsing a vector: Strings](http://r4ds.had.co.nz/data-import.html#readr-strings) for how to use locale.
I'll define a new locale using the "Latin1" encoding (equivalent to ISO-8859-1, but more descriptive),
```{r}
locale_latin1 <- locale(encoding = "Latin1")
wdi_series <- read_csv(wdi("Series"), locale = locale_latin1)
```
Now it works!
```{r}
glimpse(wdi_series)
```
The key for this table is `Series Code`, and it has a variety of metadata on the series. `Indicator name` is a human friendly label for the variable.


2. Load the country-level CSV file. Print out the full names of the countries. What's wrong with them Use the `readr::guess_encoding` function to find the probable encoding, adjust the `locale` to be able to load the data in the correct format.
```{r results='hide'}
wdi_country$`Long Name`
```
If you scan through the output you'll notice some names that have errors.
Here are a few country names with problems.
```{r}
wdi_country$`Long Name`[c(40, 49, 216)]
```
They should read "Republic of Côte d'Ivoire", "Curaçao", and "Democratic Republicn of São Tomé and Principé"), but instead they have symbols like `\xf4`.
A leading `\x` usually refers to a [hexadecimal number](https://en.wikipedia.org/wiki/Hexadecimal).
In this case it is the hexadecimal number of the character which the encoding can't recognize.
Often, googling for these hexadecimal numbers will let you figure out what it should be, and perhaps the encoding (you probably aren't the first person to encounter that problem).
Other ways in which encoding problems are indicated (not in R) could be with a leading `\u` for a unicode character, or the use of the symbol � or the empty box (for missing emoji). 

To figure out what the encoding is, I'll again use `guess_encoding`:
```{r}
guess_encoding(wdi("Country"))
```
This file is guessed to be [Windows-1252](https://en.wikipedia.org/wiki/Windows-1252) (sometimes called CP-1252 where CP stands for "code point").
But that is different than the previous file, which was encoded as ISO-1859-1.

WTF? Are the files encoded differently? 
Now I'm paranoid. I should check all the CSV files to see their encodings.
```{r}
for (filename in dir(wdi_dir, full.names = TRUE)) {
  cat(filename, "\n")
  print(guess_encoding(filename))
}
```
Bug after reading through the Wikpedia page and some additional Googling, I learn that Windows-1252 is a *superset* of ISO-1859-1.
This means that we lose nothing by specifying Windows-1252, and a few files must use one of the characters that appears in Windows-1252 and not ISO-1859-1.
So I can set the encoding to `Windows-1252` for all the CSV files.

This may seem terribly complicated, and I don't envy the engineers who have had to figure out and maintain all this.
But, what you just learned will cover 99% of all your encoding issues as long as you are working with English or Western European language texts.

Try UTF-8. If there are problems with loading it using the default UTF-8, it was probably created by some Windows program that used Windows-1252.
Try that. 
If Windows-1252 doesn't work, then you have a problem.
However, you are working with Chinese, Japanese, Arabic, Cyrillic, ... that's another story).

As before, I'll create a new `locale` object with Windows-1252 encoding.
```{r}
locale_windows1252 <- locale(encoding = "Windows-1252")
```
I'll need to reload all the files using that new encoding.
I could load each file separately,
```{r}
wdi_country <- read_csv(wdi("Country"), locale = locale_windows1252)
wdi_data <- read_csv(wdi("CS_Notes"), locale = locale_windows1252)
# ... and so on
```
But this is seeming redundant, I am continually calling `read_csv(wdi(name), locale = locale_windows1252)`.
As R4DS says, if you copy something three or more times, you should write a function.
So here it is,
```{r}
read_wdi_csv <- function(file, ...) {
  read_csv(file, ..., locale = locale(encoding = "Windows-1252"))
}
```
The `...` passes arguments to `read_csv`, which is useful if it turns out that a specific file needs to be treated differently.
Now I can do this:
```{r}
wdi_country <- read_wdi_csv(wdi("Country"))
wdi_cs_notes <- read_wdi_csv(wdi("CS_Notes"))
wdi_data <- read_wdi_csv(wdi("Data"))
wdi_description <- read_wdi_csv(wdi("Description"))
wdi_footnotes <- read_wdi_csv(wdi("Footnotes"))
wdi_series <- read_wdi_csv(wdi("Series"))
wdi_st_notes <- read_wdi_csv(wdi("ST_Notes"))
```

But even that seems annoying and redundant. 
I already have a list of filenames, and I want to name them similarly to their filenames. 
I will load them using the same function, so I really should be using a loop, which would look something like this.
```
for (filename in filelist) {
   nameofvar <- read_csv(filename, locale = locale_windows1252
}
```
I can get the names of the files using `dir`.
But how do I programmatically generate the variable names. 
The answer is to put them in a list, and then refer to the list.
```{r}
# create
filenames <- dir(wdi_dir, full.names = TRUE)
WDI <- vector("list", length(filenames))
wdi_names <- vector("character", length(filenames))
# for filename in filenames is too confusing
for (i in seq_along(filenames)) {
  fn <- filenames[i]
  # get only the file-part of the file
  name <- basename(fn)
  # extract the name part
  name <- str_match(name, "WDI_(.*)\\.csv")[1, 2]
  # I want the names to be lower-cased
  name <- str_to_lower(name)
  WDI[[i]] <- read_wdi_csv(fn)
  wdi_names[[i]] <- name
}
names(WDI) <- wdi_names
```
*Note* I had to adjust this code several times to get the looping and naming correct. 

To see what was going on inside the loop to generate the names of the lis elements:
```{r}
fn <- "data/WDI/WDI_Country.csv"
name <- basename(fn)
print(name)
str_match(name, "WDI_(.*)\\.csv")
name <- str_match(name, "WDI_(.*)\\.csv")[1, 2]
name
str_to_lower(name)
```

Now I have a list of tibbles, with names based off their CSV filenames.
```{r}
names(WDI)
```
```{r}
glimpse(WDI[["country"]])
```

This may have been overkill for this particular case.
It is also the result of me cleaning data for many years so I've developed certain generalizable patterns.
But where you can, start to use parts of the pipeline above, such as writing functions for reusable parts of the data-processing pipeline, Loading files in a loop or iteratively rather than copying and pasting.


4. Is this tidy data? Why or why not? Why do you think they organized the data this way?

5. Create a tidy dataset in which the rows correspond to (country, year) tuples, and `country`, `year`, and all the indicators are columns.

It's not tidy. If the rows should be (country-year) observations, and the columns the series. 

This creates a data-frame with four columns: country, series, year, and the value.
```{r}
wdi_data_cst <-
  WDI$data %>%
  select(-`Indicator Name`, `Country Name`) %>%
  gather(year, value, matches("^[0-9][0-9][0-9][0-9]$"),
         na.rm = TRUE)

```
We'll see later that this format can be useful for (1) simultaneously summarizing many varaibles, or (2) plotting multiple variables in ggplot.

When selecting the columns to gather I used the `matches` function which selects columns by a regular expression pattern (See the R4DS chapter [Strings](http://r4ds.had.co.nz/strings.html)).
In this case, I wanted to select columns which had names consisting of four-numbers.

There are a few other options I could have used for the `gather` expression. 
This selects everything but the country and indicator variables.
```{r eval=FALSE}
gather(year, value, -`Country Name`, -`Indicator Code`)
```
But what if I didn't drop `Indicator Name`, and `Country Name`? The code would break.

You may be wondering why I dropped `Indicator Name` and `Country Name`.
I dropped `Indicator Name` because it would not work when I would need to spread the data to make the series into separate columns.
I dropped `Country Name` since later, I will merge the country data from `WDI_Country.csv`, and that has multiple country name variables.

So I could have manually entered the years, and done so using a range:
```{r eval=FALSE}
gather(year, value, `1960`:`2016`) 
```
However, this requires that I know the range of years, and it could break if I ran the code again next year, when there will be a column for 2017.

But why did the World Bank set up the data like this? 
Recall the other files that are provided.
Some of them have metadata on series, country-series, and countries.
With the data in the original format we can merge series metadata
from `WDI_Series` with `WDI_Data`:
```{r results='hide'}
left_join(WDI$data, WDI$series,
          by = c("Indicator Code" = "Series Code"))
```
We couldn't do that if the years were in rows, and the series were seperate columns. 
So for some manipulations, the original format is better, and for others the tidy format will be better.
What is not cool is naming the column "Indicator Code" in one data set and "Series Code" in the other.

I specified the option `na.rm = TRUE` to `gather()` in order to drop missing values since this is a larger dataset.

To make the tidy dataset, call `spread` in order to make a column for each value in `Indicator Code`, filled in with the numbers in the `value` column.
```{r}
wdi_data_tidy <- wdi_data_cst %>%
  spread(`Indicator Code`, value)
dim(wdi_data_tidy)
```


5. Merge the tidy data with the country-level data. 

```{r}
wdi_data_tidy <- 
  left_join(wdi_data_tidy, WDI$country, by = "Country Code")
```
Sometimes you have to worry about whether the indicators are the same or if everything

```{r}
wdi_data_tidy %>%
  count(`Country Code`) %>%
  anti_join(WDI$country, by = "Country Code")
```

By the way, although `left_join` will merge two data frames using any variables that appear in both, you should *always* explicitly specify an argument in `by`. 
There are at least two reasons to do this: (1) it makes the code self-documenting.
Which gives you a better idea of what the code is doing?
```{r eval=FALSE}
left_join(wdi_data_tidy, WDI$country)
```
or
```{r eval=FALSE}
left_join(wdi_data_tidy, WDI$country, by = "Country Code")
```
In the former, I'd probably include a comment as to what I was merging on:
```{r}
# merge the tidy data to country level data by country codes
left_join(wdi_data_tidy, WDI$country)
```
But that is not a good comment. 
If code is clearly written, it should convey *what* it is doing.
The example with `by = "Country Code"` accomplishes that.
And even unclear code, if you spend enough time working through it, you should be able to find out *what* it does.
Comments should primarily be used for explainging *why* you are doing it.
```{r eval=FALSE}
# get descriptive country names for labelling plots and the income categories
left_join(wdi_data_tidy, WDI$country, by = "Country Code")
```
I may be able to infer why I merged country data from what I do with it later, but this is harder to understand.

   
6. The WDI data include "country" observations that are actually aggregations of countries, for example, "OECD members", "High income", "Sub-Saharan Africa (all income levels)". Filter out those observations. Hint: there is a variable that is missing for non-countries, so you shouldn't need to enumerate all of the non-country values.

Once I realized that the WDI data included non-countries, I guessed there must be an indicator in the country data which I could use to filter out non-countries.
Unforuntately, there wasn't a variable that directly measured this.
So then I started looking for variables that only make sense for countries and would be missing for the aggregates.
I found that `Income Group` was missing for all the aggregates, and non missing for countries.
```{r}
filter(WDI$country, is.na(`Income Group`))[["Short Name"]]
```

Now that I've found a variable that identifies the non-countries, I can filter them out.
This is a case where I'd want to make a comment in a script as to why I was doing this, because the thing I want to accomplish is not at all obvious from what I'm doing in the code.
In a few months (or hours) I'd look back and wonder, why was I dropping observations with a missing income group, especially if the analysis never ends up using that variable.
```{r}
wdi_data_tidy <- 
  # remove aggregate rows (e.g. Euro Area, World, etc) and
  # keep only countries. Only countries have non-missing `Income Group`
  filter(wdi_data_tidy, !is.na(`Income Group`))
```


7. Choose a few variables you are interested. 

    - Summarize the distributions of these variables by year and income group: mean, median, min, max, standard deviation, 25th percentile, 75th percentile. Accomplish by first using `gather` to create a data frame with columns: `country`, `year`, `indicator`, and `value`. Then you can use `group_by()` and `summarize()` to summarize the data.
    
    - Accomplish the same task using `mutate_at` without using `gather`
    
    - Create a line plot in which the x-axis is years, the y-axis is the mean of the indicator, each income group has its own line and a different color, and faceted by indicator. Merge the indicator level dataset in order to give the indicators meaningful labels in the plot.
      

<!--
### HDI

Calculate the Human Development Indicator. The definition can be found here: http://hdr.undp.org/sites/default/files/hdr2015_technical_notes.pdf.
Check your work against example data in the documentation.
Check your work against the true values of the [Human Development Index](http://hdr.undp.org/en/data):
-->

## Transparency and Missing Values

Hollyer et al. (2011) calculates a measure of governmental transparency using the number of missing values in the WDI.[^hollyer]
We'll calculate something similar.

1. For each country-year calculate a similar figure. Calculate a similar index of missingness. Consider the following: would you use all variables? If not, how would you select those variables? Would you calculate missing values in all variables and all time periods equally?

2. Which countries are least transparent?

3. How do transparency relate to income?

4. Subset by income and plot.

5. Recently the UN released a variable on the "Overall measure of statistical capacity". How does the (original) HRV transparency relate to that measure?
       
[^hollyer]: See the [HRV Transparency Project](http://0001c70.wcomhost.com/wp2/papers/) for more recent  information on it; a 2014 paper of theirs in *Political Analysis* provides a more rigorous measure. 


## Joining WDI to VDEM data

The Varieties of Democracy project is a recent project to develop indicators of democracy.  We would like to merge it with the WDI indicators. Download [Version 6.2](https://www.v-dem.net/en/data/data-version-6-2/) County-Year dataset.

1. What are the terms of using the data? (Why doesn't this document include a script to automatically download it?) How do you cite the data?

2. What are the primary VDEM indices? Subset the data to include only those indices and country and year identifiers.

3. Merge the VDEM and WDI data. What does VDEM use as its country-level indicator? What does the WDI use? Note: The **countrycode** package may be useful here.

4. What is the relationship between democracy and transparency?
   What is the relationship between democracy and transparency comparing only countries within a given income level?
